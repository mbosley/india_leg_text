{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "run-tbip.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPLxd2CfxpOLr86mQpufWj1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbosley/india_leg_text/blob/main/code/run_tbip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGCjmkDlPGdw"
      },
      "source": [
        "# Text-Based Ideal Points\n",
        "### ___Keyon Vafa, Suresh Naidu, David Blei___\n",
        "\n",
        "**IMPORTANT:** To save this code and your results, make sure you copy to your personal Google Drive. Under \"File\", select \"Save a copy in Drive\".\n",
        "\n",
        "Use this Colab notebook to run a Tensorflow implementation of the [text-based ideal point model (TBIP)](https://www.aclweb.org/anthology/2020.acl-main.475/) on a corpus of political text. Our [Github is more complete](https://github.com/keyonvafa/tbip), and it has code that can be used to reproduce all of our experiments. However, the TBIP is fastest on GPU, so if you do not have access to a GPU you can use Colab's GPUs for free. \n",
        "\n",
        "_Last updated: May 28, 2020_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F74rT-EG_X3t"
      },
      "source": [
        "The [TBIP](https://www.aclweb.org/anthology/2020.acl-main.475/) is an unsupervised probabilistic topic model that analyzes texts to quantify the political positions of its authors. The model does not use political parties or votes, nor does it require any text labelled by ideology. Given a corpus of political text and the authors of each document, the TBIP estimates the latent political positions of the authors of texts and how per-topic word choice changes as a function of the political position of the author (\"ideological topics\"). [Refer to our paper for more information](https://www.aclweb.org/anthology/2020.acl-main.475/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpeZbYNhFV1u"
      },
      "source": [
        "The default corpus for this Colab notebook is [Senate speeches](https://data.stanford.edu/congress_text) from the 114th Senate session (2015-2017), but you can also upload your own data (we provide instructions below). [In our paper](https://www.aclweb.org/anthology/2020.acl-main.475/), we use the following corpora: Senate speeches, tweets from senators, and tweets from 2020 Democratic presidential candidates. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOKO34yCHoyh"
      },
      "source": [
        "## Getting started\n",
        "\n",
        "First, **make sure you are running this Colab using a GPU**. Go to the \"Runtime\" menu, and click \"Change runtime type\". If the \"Hardware accelerator\" is listed as \"None\" or \"TPU\", change to \"GPU\". Click \"Save\" and you're ready to go. Also, as described in the first cell, make sure this code is copied to your personal Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG2sd0ngI6bW"
      },
      "source": [
        "## Dependency imports\n",
        "\n",
        "We start with dependency imports. Our code uses Tensorflow 1.x and the Tensorflow Probability library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnQzKpvkUO8z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69c3b90d-72cd-4672-f1fe-443601dee500"
      },
      "source": [
        "import functools\n",
        "import time\n",
        "import os\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import numpy as np\n",
        "import scipy.sparse as sparse\n",
        "from sklearn.decomposition import NMF\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLtdl5i-JuKR"
      },
      "source": [
        "Below we clone our [Github repo for the TBIP](https://github.com/keyonvafa/tbip). The main inference code lives in this repo, as does the Senate speeches corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCd70v6aUUW7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7173be19-8eef-465a-84d7-afc514a4fd00"
      },
      "source": [
        "! git clone https://github.com/keyonvafa/tbip\n",
        "import tbip.tbip as tbip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'tbip'...\n",
            "remote: Enumerating objects: 56, done.\u001b[K\n",
            "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 187 (delta 31), reused 14 (delta 6), pack-reused 131\u001b[K\n",
            "Receiving objects: 100% (187/187), 61.75 MiB | 27.00 MiB/s, done.\n",
            "Resolving deltas: 100% (82/82), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPiupsoLKK1l"
      },
      "source": [
        "## Hyperparameters and Initialization\n",
        "\n",
        "We start setting some hyperparameters. We fix the number of topics $K = 50$. We also set a random seed for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKNwuA_0UVrU"
      },
      "source": [
        "num_topics = 50\n",
        "tf.set_random_seed(0)\n",
        "random_state = np.random.RandomState(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyEnjWiEMr-T"
      },
      "source": [
        "The next cell provides the data directory. The directory in the cell below links to speeches from the 114th Senate session from the `tbip` repo.\n",
        "\n",
        "To use your own corpus, upload the following four files to the Colab working directory:\n",
        "\n",
        "* `counts.npz`: a `[num_documents, num_words]` [sparse CSR matrix](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html) containing the word counts for each document.\n",
        "* `author_indices.npy`: a `[num_documents]` vector where each entry is an integer in the set `{0, 1, ..., num_authors - 1}`, indicating the author of the corresponding document in `counts.npz`.\n",
        "* `vocabulary.txt`: a `[num_words]`-length file where each line denotes the corresponding word in the vocabulary.\n",
        "* `author_map.txt`: a `[num_authors]`-length file where each line denotes the name of an author in the corpus.\n",
        "\n",
        "See [Senate speech clean data](https://github.com/keyonvafa/tbip/tree/master/data/senate-speeches-114/clean) for an example of what the four files look like for Senate speeches. [Our setup script](https://github.com/keyonvafa/tbip/blob/master/setup/senate_speeches_to_bag_of_words.py) \n",
        "contains example code for creating the four files from unprocessed data for Senate speeches.\n",
        "\n",
        "**IMPORTANT:** If you are using your own corpus, change the following line to `data_dir = '.'` after uploading the four files to the Colab working directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMKwz-SRUYhp"
      },
      "source": [
        "data_dir = 'tbip/data/senate-speeches-114/clean'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsVdriAIPnML"
      },
      "source": [
        "[In our paper](https://www.aclweb.org/anthology/2020.acl-main.475/), we pre-initialize the parameters $\\theta$ and $\\beta$ with [Poisson factorization](https://arxiv.org/abs/1311.1704). Most of the time, we find this doesn't make a big difference for the learned ideal points, but it helps us interpret the ideological topics. \n",
        "\n",
        "Below, we initialize with Scikit-Learn's non-negative matrix factorization (NMF) implementation. Although we find that Poisson factorization learns more interpretable topics, we use Scikit-Learn's NMF implementation here because it is faster. To use Poisson factorization, see our [code in the Github repo](https://github.com/keyonvafa/tbip/blob/master/setup/poisson_factorization.py). \n",
        "\n",
        "If you would like to skip this pre-initialization step, set `pre_initialize_parameters = False` in the cell below. (Pre-initialization is recommended.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9UOM0edKw6c"
      },
      "source": [
        "pre_initialize_parameters = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51PBneYMQuv9"
      },
      "source": [
        "If you are pre-initializing parameters, the following cell might take a minute or so."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLgkfNNZUZv5"
      },
      "source": [
        "counts = sparse.load_npz(os.path.join(data_dir, 'counts.npz'))\n",
        "num_documents, num_words = counts.shape\n",
        "if pre_initialize_parameters:\n",
        "  nmf_model = NMF(n_components=num_topics, \n",
        "                  init='random', \n",
        "                  random_state=0, \n",
        "                  max_iter=500)\n",
        "  # Add offset to make sure none are zero.\n",
        "  initial_document_loc = np.float32(nmf_model.fit_transform(counts) + 1e-3)\n",
        "  initial_objective_topic_loc = np.float32(nmf_model.components_ + 1e-3)\n",
        "\n",
        "else:\n",
        "  initial_document_loc = np.float32(\n",
        "      np.exp(random_state.randn(num_documents, num_topics)))\n",
        "  initial_objective_topic_loc = np.float32(\n",
        "      np.exp(random_state.randn(num_topics, num_words)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q76xCfnzTsfF"
      },
      "source": [
        "## Create Iterator for Training\n",
        "\n",
        "Below we build our input pipeline for training. We train with data subsampling. We set a batch size of 512; we recommend to use the largest batch size that fits in memory. After we create the input pipeline, `counts` is the `[batch_size, num_words]` tensor representing word counts for each document, `document_indices` is the `[batch_size]` tensor of integers representing the indices of each document in the batch, and `author_indices` is the `[batch_size]` tensor of integers representing the indices of each author in the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc-DJqgeUbgl"
      },
      "source": [
        "batch_size = 512\n",
        "(iterator, author_weights, vocabulary, author_map, \n",
        " num_documents, num_words, num_authors) = tbip.build_input_pipeline(\n",
        "      data_dir, \n",
        "      batch_size,\n",
        "      random_state,\n",
        "      counts_transformation='nothing')\n",
        "document_indices, counts, author_indices = iterator.get_next()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNnPl-gOUyTN"
      },
      "source": [
        "## Perform Inference\n",
        "\n",
        "We perform inference using [variational inference](https://arxiv.org/abs/1601.00670) with [reparameterization](https://arxiv.org/abs/1312.6114) [gradients](https://arxiv.org/abs/1401.4082). We provide a brief summary below, but encourage readers to [refer to our paper](https://www.aclweb.org/anthology/2020.acl-main.475/) for a more complete overview.\n",
        "\n",
        "It is intractable to evaluate the posterior distribution $p(\\theta, \\beta, \\eta, x | y)$, so we approximate the posterior with a distribution $q_\\phi(\\theta, \\beta,\\eta,x)$, parameterized by $\\phi$. How do we set the values $\\phi$? We want to minimize the KL-Divergence between $q$ and the posterior, which is equivalent to maximizing the ELBO:\n",
        "$$\\mathbb{E}_{q_\\phi}[\\log p(y, \\theta, \\beta, \\eta, x) - \\log q_{\\phi}(\\theta, \\beta, \\eta, x)].$$\n",
        "We set the variational family to be the mean-field family, meaning the latent variables factorize over documents $d$, topics $k$, and authors $s$:\n",
        "$$q_\\phi(\\theta, \\beta, \\eta, x) = \\prod_{d,k,s} q(\\theta_d)q(\\beta_k)q(\\eta_k)q(x_s).$$\n",
        "We use lognormal factors for the positive variables and Gaussian factors for the real variables:\n",
        "$$q(\\theta_d) = \\text{LogNormal}_K(\\mu_{\\theta_d}\\sigma^2_{\\theta_d})$$\n",
        "$$q(\\theta_d) = \\text{LogNormal}_V(\\mu_{\\beta_k}, \\sigma^2_{\\beta_k})$$\n",
        "$$q(\\eta_k) = \\mathcal{N}_V(\\mu_{\\eta_k}, \\sigma^2_{\\eta_k})$$\n",
        "$$q(x_s) = \\mathcal{N}(\\mu_{x_s}, \\sigma^2_{x_s}).$$\n",
        "\n",
        "Thus, our goal is to maximize the ELBO with respect to $\\phi = \\{\\mu_\\theta, \\sigma_\\theta, \\mu_\\beta, \\sigma_\\beta,\\mu_\\eta, \\sigma_\\eta, \\mu_x, \\sigma_x\\}$. \n",
        "\n",
        "In the cells below, we initialize the variational parameters $\\phi$ and their respective variational distributions. We use `loc` to name the location variables $\\mu$, and `scale` to name the scale variables $\\sigma$. So, $\\mu_\\eta$ is denoted by `ideological_topic_loc` and $\\sigma_\\eta$ is denoted by `ideological_topic_scale`, etc. Its corresponding variational distribution is `ideological_topic_distribution`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knFyutOoUgtj"
      },
      "source": [
        "# Create Lognormal variational family for document intensities (theta).\n",
        "document_loc = tf.get_variable(\n",
        "    \"document_loc\",\n",
        "    initializer=tf.constant(np.log(initial_document_loc)))\n",
        "document_scale_logit = tf.get_variable(\n",
        "    \"document_scale_logit\",\n",
        "    shape=[num_documents, num_topics],\n",
        "    initializer=tf.initializers.random_normal(mean=-2, stddev=1.),\n",
        "    dtype=tf.float32)\n",
        "document_scale = tf.nn.softplus(document_scale_logit)\n",
        "document_distribution = tfp.distributions.LogNormal(\n",
        "    loc=document_loc,\n",
        "    scale=document_scale) \n",
        "\n",
        "# Create Lognormal variational family for objective topics (beta).\n",
        "objective_topic_loc = tf.get_variable(\n",
        "    \"objective_topic_loc\",\n",
        "    initializer=tf.constant(np.log(initial_objective_topic_loc)))\n",
        "objective_topic_scale_logit = tf.get_variable(\n",
        "    \"objective_topic_scale_logit\",\n",
        "    shape=[num_topics, num_words],\n",
        "    initializer=tf.initializers.random_normal(mean=-2, stddev=1.),\n",
        "    dtype=tf.float32)\n",
        "objective_topic_scale = tf.nn.softplus(objective_topic_scale_logit)\n",
        "objective_topic_distribution = tfp.distributions.LogNormal(\n",
        "    loc=objective_topic_loc,\n",
        "    scale=objective_topic_scale)\n",
        "  \n",
        "# Create Gaussian variational family for ideological topics (eta).\n",
        "ideological_topic_loc = tf.get_variable(\n",
        "    \"ideological_topic_loc\",\n",
        "    shape=[num_topics, num_words],\n",
        "    dtype=tf.float32)\n",
        "ideological_topic_scale_logit = tf.get_variable(\n",
        "    \"ideological_topic_scale_logit\",\n",
        "    shape=[num_topics, num_words],\n",
        "    dtype=tf.float32)\n",
        "ideological_topic_scale = tf.nn.softplus(ideological_topic_scale_logit)\n",
        "ideological_topic_distribution = tfp.distributions.Normal(\n",
        "    loc=ideological_topic_loc,\n",
        "    scale=ideological_topic_scale)\n",
        "\n",
        "# Create Gaussian variational family for ideal points (x).\n",
        "ideal_point_loc = tf.get_variable(\n",
        "    \"ideal_point_loc\",\n",
        "    shape=[num_authors],\n",
        "    dtype=tf.float32)\n",
        "ideal_point_scale_logit = tf.get_variable(\n",
        "    \"ideal_point_scale_logit\",\n",
        "    initializer=tf.initializers.random_normal(mean=0, stddev=1.),\n",
        "    shape=[num_authors],\n",
        "    dtype=tf.float32)\n",
        "ideal_point_scale = tf.nn.softplus(ideal_point_scale_logit)\n",
        "ideal_point_distribution = tfp.distributions.Normal(\n",
        "    loc=ideal_point_loc,\n",
        "    scale=ideal_point_scale)\n",
        "\n",
        "# Approximate ELBO.\n",
        "elbo = tbip.get_elbo(counts,\n",
        "                     document_indices,\n",
        "                     author_indices,\n",
        "                     author_weights,\n",
        "                     document_distribution,\n",
        "                     objective_topic_distribution,\n",
        "                     ideological_topic_distribution,\n",
        "                     ideal_point_distribution,\n",
        "                     num_documents,\n",
        "                     batch_size)\n",
        "loss = -elbo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geoqKLFtY4g6"
      },
      "source": [
        "## Optimize\n",
        "\n",
        "We train using the Adam optimizer. To change the learning rate, change the value of `learning_rate` below.\n",
        "\n",
        "The tensors `neutral_mean`, `positive_mean`, and `negative_mean` provide ideological topics for the topics under discussion in the corpus. `neutral_mean` contains the topic estimates for an author $s$ with an ideal point $x_s=0$, `positive_mean` contains the topics for an author $s$ with an ideal point $x_s = 1$, and `negative_mean` contains the topics for an author $s$ with an ideal point $x_s = -1$. We print these topics as we train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhp5kpgzYsSr"
      },
      "source": [
        "learning_rate = 0.01\n",
        "optim = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optim.minimize(loss)\n",
        "\n",
        "neutral_mean = objective_topic_loc + objective_topic_scale ** 2 / 2\n",
        "positive_mean = (objective_topic_loc + \n",
        "                 ideological_topic_loc + \n",
        "                 (objective_topic_scale ** 2 + \n",
        "                  ideological_topic_scale ** 2) / 2)\n",
        "negative_mean = (objective_topic_loc - \n",
        "                 ideological_topic_loc +\n",
        "                 (objective_topic_scale ** 2 + \n",
        "                  ideological_topic_scale ** 2) / 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYbRuvvqUivQ",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to create helper functions for printing topics and ordered ideal points\n",
        "\n",
        "def get_topics(neutral_mean, \n",
        "               negative_mean, \n",
        "               positive_mean, \n",
        "               vocabulary, \n",
        "               print_to_terminal=True):\n",
        "  num_topics, num_words = neutral_mean.shape\n",
        "  words_per_topic = 10\n",
        "  top_neutral_words = np.argsort(-neutral_mean, axis=1)\n",
        "  top_negative_words = np.argsort(-negative_mean, axis=1)\n",
        "  top_positive_words = np.argsort(-positive_mean, axis=1)\n",
        "  topic_strings = []\n",
        "  for topic_idx in range(num_topics):\n",
        "    neutral_start_string = \"Neutral  {}:\".format(topic_idx)\n",
        "    neutral_row = [vocabulary[word] for word in \n",
        "                    top_neutral_words[topic_idx, :words_per_topic]]\n",
        "    neutral_row_string = \", \".join(neutral_row)\n",
        "    neutral_string = \" \".join([neutral_start_string, neutral_row_string])\n",
        "    \n",
        "    positive_start_string = \"Positive {}:\".format(topic_idx)\n",
        "    positive_row = [vocabulary[word] for word in \n",
        "                    top_positive_words[topic_idx, :words_per_topic]]\n",
        "    positive_row_string = \", \".join(positive_row)\n",
        "    positive_string = \" \".join([positive_start_string, positive_row_string])\n",
        "    \n",
        "    negative_start_string = \"Negative {}:\".format(topic_idx)\n",
        "    negative_row = [vocabulary[word] for word in \n",
        "                    top_negative_words[topic_idx, :words_per_topic]]\n",
        "    negative_row_string = \", \".join(negative_row)\n",
        "    negative_string = \" \".join([negative_start_string, negative_row_string])\n",
        "    \n",
        "    if print_to_terminal:\n",
        "      topic_strings.append(negative_string)\n",
        "      topic_strings.append(neutral_string)\n",
        "      topic_strings.append(positive_string)\n",
        "      topic_strings.append(\"==========\")\n",
        "    else:\n",
        "      topic_strings.append(\"  \\n\".join(\n",
        "        [negative_string, neutral_string, positive_string]))\n",
        "  \n",
        "  if print_to_terminal:\n",
        "    all_topics = \"{}\\n\".format(np.array(topic_strings))\n",
        "  else:\n",
        "    all_topics = np.array(topic_strings)\n",
        "  return all_topics\n",
        "\n",
        "\n",
        "def get_ideal_points(ideal_point_loc, author_map, print_to_terminal=True):\n",
        "  \"\"\"Print ideal point ordering for Tensorboard.\"\"\"\n",
        "  if print_to_terminal:\n",
        "    offset = 5\n",
        "    sorted_authors = author_map[np.argsort(ideal_point_loc)]\n",
        "    authors_by_line = [\", \".join(sorted_authors[i * offset:i*offset+offset]) \n",
        "                       for i in range((len(author_map) - 1) // offset + 1)]\n",
        "    sorted_list = (\"Sorted ideal points:\"\n",
        "        \"\\n ==================== \\n{}\"\n",
        "        \"\\n ==================== \".format(\",\\n\".join(authors_by_line)))\n",
        "  else:\n",
        "    sorted_list = \", \".join(author_map[np.argsort(ideal_point_loc)])\n",
        "  \n",
        "  return sorted_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY9Cz2fKaCZ1"
      },
      "source": [
        "## Execute Training\n",
        "\n",
        "The code above was creating the graph; below we actually run training. You can adjust the number of steps to train (`max_steps`) and the frequency at which to print the ELBO (`print_steps`) in the cell below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw5NWUKH2n_n"
      },
      "source": [
        "max_steps = 50000\n",
        "print_steps = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHuSetw42hiM"
      },
      "source": [
        "Here, we run our training loop. Topic summaries and ordered ideal points will print every 2500 steps. Typically in our experiments it takes 15,000 steps or so to begin seeing sensible results, but of course this depends on the corpus. These sensible results should be reached within a half hour. For the default corpus of Senate speeches, it should take less than 2 hours to complete the full 50,000 training steps. \n",
        "\n",
        "**NOTE:** You may see a RAM error while training. This is fine; the script should finish training. Although you will lose all local variables, parameters are saved while training. To access these parameters, restart the notebook when training is finished and jump to the section \"Analyze Results\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvoS73h7Uj8I"
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "start_time = time.time()\n",
        "for step in range(max_steps):\n",
        "  (_, elbo_val) = sess.run([train_op, elbo])\n",
        "  duration = (time.time() - start_time) / (step + 1)\n",
        "  if step % print_steps == 0 or step == max_steps - 1:\n",
        "    print(\"Step: {:>3d} ELBO: {:.3f} ({:.3f} sec/step)\".format(\n",
        "        step, elbo_val, duration))\n",
        "  if (step + 1) % 2500 == 0 or step == max_steps - 1:\n",
        "    (neutral_topic_mean, negative_topic_mean, positive_topic_mean, \n",
        "     ideal_point_mean) = sess.run([neutral_mean, negative_mean, \n",
        "                                   positive_mean, ideal_point_loc])\n",
        "    print(get_topics(neutral_topic_mean, \n",
        "                     negative_topic_mean, \n",
        "                     positive_topic_mean, \n",
        "                     vocabulary))\n",
        "    print(get_ideal_points(ideal_point_mean, author_map))\n",
        "    np.save(\"neutral_topic_mean.npy\", neutral_topic_mean)\n",
        "    np.save(\"negative_topic_mean.npy\", negative_topic_mean)\n",
        "    np.save(\"positive_topic_mean.npy\", positive_topic_mean)\n",
        "    np.save(\"ideal_point_mean.npy\", ideal_point_mean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDzXBr-__UGp"
      },
      "source": [
        "## Analyze Results\n",
        "\n",
        "**NOTE:** Even if running the TBIP training results in a RAM error, you can start right at this cell since the results are saved.\n",
        "\n",
        "Below we load the ideal points and ideological topics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArN9eHBSUoi3"
      },
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "ideal_point_mean = np.load(\"ideal_point_mean.npy\")\n",
        "neutral_topic_mean = np.load(\"neutral_topic_mean.npy\")\n",
        "negative_topic_mean = np.load(\"negative_topic_mean.npy\")\n",
        "positive_topic_mean = np.load(\"positive_topic_mean.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0MAzaedLCIj"
      },
      "source": [
        "Now we load our list of authors. If you used your own corpus, change the following line to `data_dir = '.'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CR9dE27nLBmK"
      },
      "source": [
        "data_dir = 'tbip/data/senate-speeches-114/clean'\n",
        "author_map = np.loadtxt(os.path.join(data_dir, 'author_map.txt'), \n",
        "                        dtype=str, \n",
        "                        delimiter='\\n', \n",
        "                        comments='//')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nsz1uMW3oIsE"
      },
      "source": [
        "For example, here is a graph of the learned ideal points. We don't label each point because there are too many to plot. Below we select some authors to label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qip3-dODoFP4"
      },
      "source": [
        "selected_authors = np.array(\n",
        "    [\"Bernard Sanders (I)\", \"Elizabeth Warren (D)\", \"Charles Schumer (D)\", \n",
        "     \"Susan Collins (R)\", \"Marco Rubio (R)\", \"John Mccain (R)\", \n",
        "     \"Ted Cruz (R)\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tqh6j2yMhZrs"
      },
      "source": [
        "sns.set(style=\"whitegrid\")\n",
        "fig = plt.figure(figsize=(12, 1))\n",
        "ax = plt.axes([0, 0, 1, 1], frameon=False)\n",
        "for index in range(len(author_map)):\n",
        "  ax.scatter(ideal_point_mean[index], 0, c='black', s=20)\n",
        "  if author_map[index] in selected_authors:\n",
        "      ax.annotate(author_map[index], xy=(ideal_point_mean[index], 0.), \n",
        "                  xytext=(ideal_point_mean[index], 0), rotation=30, size=14)\n",
        "ax.set_yticks([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lT2Trkc7avB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}